{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:24:03.516971Z","iopub.status.busy":"2024-07-10T05:24:03.516734Z","iopub.status.idle":"2024-07-10T05:24:05.629001Z","shell.execute_reply":"2024-07-10T05:24:05.628015Z","shell.execute_reply.started":"2024-07-10T05:24:03.516947Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:24:07.573108Z","iopub.status.busy":"2024-07-10T05:24:07.572166Z","iopub.status.idle":"2024-07-10T05:24:07.640374Z","shell.execute_reply":"2024-07-10T05:24:07.639488Z","shell.execute_reply.started":"2024-07-10T05:24:07.573078Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/emotion-anal/data.csv')\n","\n","# Очистка данных\n","df['Answer'] = df['Answer'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n","\n","# Разделение данных на признаки и метки\n","X = df['Answer']\n","y = df.iloc[:, 1:]\n","\n","# Разделение данных на тренировочную и тестовую выборки\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:24:08.505938Z","iopub.status.busy":"2024-07-10T05:24:08.505081Z","iopub.status.idle":"2024-07-10T05:24:08.512896Z","shell.execute_reply":"2024-07-10T05:24:08.511995Z","shell.execute_reply.started":"2024-07-10T05:24:08.505905Z"},"trusted":true},"outputs":[],"source":["def tokenize_data(texts, labels, tokenizer, max_length=256):\n","    input_ids = []\n","    attention_masks = []\n","    \n","    for text in texts:\n","        encoded = tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","    \n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels.values, dtype=torch.float)\n","    \n","    return input_ids, attention_masks, labels"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:24:09.633439Z","iopub.status.busy":"2024-07-10T05:24:09.633066Z","iopub.status.idle":"2024-07-10T05:24:16.950342Z","shell.execute_reply":"2024-07-10T05:24:16.949341Z","shell.execute_reply.started":"2024-07-10T05:24:09.633409Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5e1b2098ed644e6b92cab346d070d99","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83858c1403634fc8babca9cc8fa5977a","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d30955274d7f4141b619b07340ac5a3c","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"924aac7e8e5e4ce7ac8f11fcb38e856c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f7eba4f0d944fb785cadfdc7a717276","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["import torch\n","from transformers import RobertaTokenizer\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","X_train_ids, X_train_masks, y_train_tensors = tokenize_data(X_train, y_train, tokenizer)\n","X_test_ids, X_test_masks, y_test_tensors = tokenize_data(X_test, y_test, tokenizer)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:24:24.214479Z","iopub.status.busy":"2024-07-10T05:24:24.213520Z","iopub.status.idle":"2024-07-10T05:24:28.046893Z","shell.execute_reply":"2024-07-10T05:24:28.046011Z","shell.execute_reply.started":"2024-07-10T05:24:24.214447Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"060a896610c44242b77363dd56ead845","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=29, bias=True)\n","  )\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import RobertaForSequenceClassification\n","\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=y_train.shape[1])\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:27:51.601635Z","iopub.status.busy":"2024-07-10T05:27:51.600548Z","iopub.status.idle":"2024-07-10T05:27:51.615636Z","shell.execute_reply":"2024-07-10T05:27:51.613871Z","shell.execute_reply.started":"2024-07-10T05:27:51.601594Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n","from transformers import AdamW\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Создание DataLoader для тренировочных данных\n","train_data = TensorDataset(X_train_ids, X_train_masks, y_train_tensors)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=8)\n","\n","# Создание DataLoader для тестовых данных\n","test_data = TensorDataset(X_test_ids, X_test_masks, y_test_tensors)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=8)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:28:05.314172Z","iopub.status.busy":"2024-07-10T05:28:05.313798Z","iopub.status.idle":"2024-07-10T05:28:05.321555Z","shell.execute_reply":"2024-07-10T05:28:05.320559Z","shell.execute_reply.started":"2024-07-10T05:28:05.314142Z"},"trusted":true},"outputs":[],"source":["def train_model(model, dataloader, optimizer, device, epochs=3):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for step, batch in enumerate(dataloader):\n","            batch_input_ids, batch_attention_masks, batch_labels = batch\n","            batch_input_ids = batch_input_ids.to(device)\n","            batch_attention_masks = batch_attention_masks.to(device)\n","            batch_labels = batch_labels.to(device)\n","            \n","            model.zero_grad()\n","            \n","            outputs = model(\n","                input_ids=batch_input_ids,\n","                attention_mask=batch_attention_masks,\n","                labels=batch_labels\n","            )\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            \n","            loss.backward()\n","            optimizer.step()\n","        \n","        avg_loss = total_loss / len(dataloader)\n","        print(f'Epoch {epoch + 1}, Loss: {avg_loss}')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:54:44.223333Z","iopub.status.busy":"2024-07-10T05:54:44.222675Z","iopub.status.idle":"2024-07-10T05:57:15.568071Z","shell.execute_reply":"2024-07-10T05:57:15.567062Z","shell.execute_reply.started":"2024-07-10T05:54:44.223300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.023406963090638857\n","Epoch 2, Loss: 0.02223952181284895\n","Epoch 3, Loss: 0.01997888132275359\n","Epoch 4, Loss: 0.019106505860297662\n","Epoch 5, Loss: 0.01794999240381593\n"]}],"source":["train_model(model, train_dataloader, optimizer, device, 5)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:54:08.955403Z","iopub.status.busy":"2024-07-10T05:54:08.954569Z","iopub.status.idle":"2024-07-10T05:54:08.962498Z","shell.execute_reply":"2024-07-10T05:54:08.961596Z","shell.execute_reply.started":"2024-07-10T05:54:08.955369Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","    \n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch_input_ids, batch_attention_masks, batch_labels = batch\n","            batch_input_ids = batch_input_ids.to(device)\n","            batch_attention_masks = batch_attention_masks.to(device)\n","            batch_labels = batch_labels.to(device)\n","            \n","            outputs = model(\n","                input_ids=batch_input_ids,\n","                attention_mask=batch_attention_masks\n","            )\n","            logits = outputs.logits\n","            predictions.append(logits.cpu().numpy())\n","            true_labels.append(batch_labels.cpu().numpy())\n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_labels = np.concatenate(true_labels, axis=0)\n","    \n","    return predictions, true_labels"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T05:57:26.455798Z","iopub.status.busy":"2024-07-10T05:57:26.455051Z","iopub.status.idle":"2024-07-10T05:57:28.765150Z","shell.execute_reply":"2024-07-10T05:57:28.764235Z","shell.execute_reply.started":"2024-07-10T05:57:26.455761Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Средняя accuracy по всем классам: 0.9309175920514319\n","Средняя F1 по всем классам:  0.515602152738415\n"]}],"source":["import numpy as np\n","from sklearn.metrics import f1_score\n","\n","predictions, true_labels = evaluate_model(model, test_dataloader, device)\n","\n","predicted_labels = (predictions > 0.5).astype(int)\n","\n","acc = []\n","f1 = []\n","for i, col in enumerate(y.columns):\n","    acc.append(accuracy_score(true_labels[:, i], predicted_labels[:, i]))\n","    f1.append(f1_score(true_labels[:, i], predicted_labels[:, i], zero_division=True))\n","\n","print(\"Средняя accuracy по всем классам:\", np.mean(acc))\n","print(\"Средняя F1 по всем классам: \", np.mean(f1))"]},{"cell_type":"markdown","metadata":{},"source":["### Выводы\n","В предыдущем уроке были обучены логистическая регрессия и SVM на фичах TF-IDF. Данные модели показали качества accuracy примерно 90%, после оптимизации гиперпараметров было достигнуто f1 0.34.  \n","\n","Дообученная NLP модель roberta после примерно 20 эпох обучения показала accuracy 93%, а f1 0.51, что означает, что модель улучшилась."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5364134,"sourceId":8919110,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
